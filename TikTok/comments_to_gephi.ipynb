{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from itertools import combinations\n",
    "from spacymoji import Emoji\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "nlp = spacy.load('it_core_news_sm')\n",
    "nlp.add_pipe(\"emoji\", first=True)\n",
    "_data_path = \"./TikTok/Tikapi/data/\"\n",
    "video_ids_file = _data_path + \"video_list.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load spreadsheet of videos\n",
    "df = pd.read_csv(video_ids_file)\n",
    "df = df.fillna(\"\")\n",
    "\n",
    "# create additional columns\n",
    "df[\"id\"] = [re.sub(\"video/\", \"\", re.findall(\"video/[0-9]{19}\", link)[0]) for link in df[\"Link\"]]\n",
    "df[\"name\"] = np.where(df[\"Politician\"]==\"\", df[\"Influencer/tiktoker\"].replace(\" \", \"_\", regex=True), df[\"Politician\"].replace(\" \", \"_\", regex=True))\n",
    "df[\"file\"] = _data_path + df['name'] + \"_com_\" + df[\"id\"] + \".json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load comments with meta data into dictionary (key = video_id)\n",
    "dic = {}\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    if os.path.isfile(row[\"file\"]):\n",
    "        with open(row[\"file\"]) as infile:\n",
    "            comments = json.load(infile)\n",
    "        dic[row[\"id\"]] = {\"raw_comments\":comments[\"comments\"], \"meta\":comments[\"meta\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently we have comments for 50 videos.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Currently we have comments for {len(dic.keys())} videos.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocessing:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def clean_text(self, text: str, search_words: list = []) -> list:\n",
    "        \"\"\" \n",
    "        Cleans a string removing punctuation, emoji, stopwords, lemmatizazion, ...\n",
    "\n",
    "        text: Input string like a sentence\n",
    "        search_words: WOrd that were use to query for the input data and should therefore be removed\n",
    "        output: bag of words\n",
    "        \"\"\"\n",
    "        doc = nlp(text)\n",
    "\n",
    "        bog = []\n",
    "\n",
    "        for token in doc:\n",
    "\n",
    "            # filter stopwords\n",
    "            if not token.is_stop:\n",
    "                # filter punctiation\n",
    "                if not token.is_punct:\n",
    "                    if not token.like_url:\n",
    "                        if not token.like_email:\n",
    "                            if not token.is_space:\n",
    "                                if token.lemma_ not in ['\\n', ' ']:\n",
    "                                    # filter words used to search for tweets  \n",
    "                                    if not token._.is_emoji:\n",
    "                                    \n",
    "                                        if str(token) not in search_words:\n",
    "                                            bog.append(re.sub('@', '', token.lemma_))                         \n",
    "        \n",
    "        return bog\n",
    "\n",
    "    \n",
    "    def create_comment_list(self, dic: dict, search_words: list = []) -> list:\n",
    "        \"\"\" \n",
    "        Creates a flat list of cleaned comments from a dictionary.\n",
    "\n",
    "        dic: dictionnary of comments\n",
    "        search_words: search words to pass to self.clean_text\n",
    "        \"\"\"\n",
    "\n",
    "        com_list = []\n",
    "\n",
    "        for key, video in dic.items():\n",
    "            for com in video[\"raw_comments\"]:\n",
    "                com_list.append(\" \".join(self.clean_text(com[\"text\"])))\n",
    "\n",
    "        return com_list\n",
    "            \n",
    "    def create_edge_dictionary(self, comments: list) -> dict:\n",
    "        \"\"\" \n",
    "        Creates edges between two words that appear in the same comment. A weight is assigned according to the number of occurences of an edge in the dataset.\n",
    "\n",
    "        comments: list of comments where each comment is a list of words\n",
    "        output: dicitonary with edges as keys and weight as values\n",
    "        \"\"\"\n",
    "        \n",
    "        weights = {}\n",
    "\n",
    "        for com in comments:\n",
    "            for edge in combinations(com.split(), 2):\n",
    "\n",
    "                # if not self loop\n",
    "                if edge[0] != edge[1]:\n",
    "                    \n",
    "                    if edge not in weights.keys():\n",
    "                        weights[edge] = 1\n",
    "                    else:\n",
    "                        weights[edge] += 1\n",
    "                    \n",
    "        return weights\n",
    "\n",
    "    def edges_to_dataframe(self, edges: dict) -> pd.DataFrame:\n",
    "        \"\"\" \n",
    "        Creates data frame of edges from an edge dictionary (self.create_edge_dictionary). This can be saved to csv to use in gephi. In python better use the dicitionary as it is much faster.\n",
    "\n",
    "        edges: dictionary of edge\n",
    "        output: pd.DataFrame of edges\n",
    "        \"\"\"\n",
    "        df = pd.DataFrame({\"source\": [], \"target\": [], \"weigth\": []})\n",
    "\n",
    "        for edge, weight in edges.items():\n",
    "            row = pd.DataFrame({\"source\": [edge[0]], \"target\": [edge[1]], \"weigth\": [weight]})\n",
    "            df = pd.concat([df, row], ignore_index=True)\n",
    "        \n",
    "        return df\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create comment lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create instance of our preprocessing class\n",
    "prepro = preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over videos in spreadsheet and clean and add comments from d\n",
    "\n",
    "com_list_all = prepro.create_comment_list(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a subdictionary via some fiter criteria from the spreadsheet (e.g. name)\n",
    "subdic_meloni = {k: dic[k] for k in df.loc[df[\"name\"]==\"Meloni\", \"id\"]}\n",
    "com_list_meloni = prepro.create_comment_list(subdic_meloni)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create & save edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create weighted edges. Two words are connected if they are in the same comment\n",
    "edges_all = prepro.create_edge_dictionary(com_list_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe of edges to be saved to csv for gephi\n",
    "edges_all_df = prepro.edges_to_dataframe(edges_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save edges to csv\n",
    "edges_all_df.to_csv(_data_path + \"edges_coms_all.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\"> <i><b>Fin</b></i> </p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "netsci",
   "language": "python",
   "name": "netsci-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
