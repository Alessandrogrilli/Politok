{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jan/ds/3rd-semester/NetSci/netsci-venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from itertools import combinations\n",
    "from spacymoji import Emoji\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "nlp = spacy.load('it_core_news_sm')\n",
    "nlp.add_pipe(\"emoji\", first=True)\n",
    "_data_path = \"./Tikapi/data/\"\n",
    "video_ids_file = _data_path + \"video_list.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load spreadsheet of videos\n",
    "df = pd.read_csv(video_ids_file)\n",
    "df = df.fillna(\"\")\n",
    "\n",
    "# create additional columns\n",
    "df[\"id\"] = [re.sub(\"video/\", \"\", re.findall(\"video/[0-9]{19}\", link)[0]) for link in df[\"Link\"]]\n",
    "df[\"name\"] = np.where(df[\"Politician\"]==\"\", df[\"Influencer/tiktoker\"].replace(\" \", \"_\", regex=True), df[\"Politician\"].replace(\" \", \"_\", regex=True))\n",
    "df[\"file\"] = _data_path + df['name'] + \"_com_\" + df[\"id\"] + \".json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load comments with meta data into dictionary (key = video_id)\n",
    "dic = {}\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    if os.path.isfile(row[\"file\"]):\n",
    "        with open(row[\"file\"]) as infile:\n",
    "            comments = json.load(infile)\n",
    "        dic[row[\"id\"]] = {\"raw_comments\":comments[\"comments\"], \"meta\":comments[\"meta\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently we have comments for 145 videos.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Currently we have comments for {len(dic.keys())} videos.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocessing:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def clean_text(self, text: str, search_words: list = []) -> list:\n",
    "        \"\"\" \n",
    "        Cleans a string removing punctuation, emoji, stopwords, lemmatizazion, ...\n",
    "\n",
    "        text: Input string like a sentence\n",
    "        search_words: WOrd that were use to query for the input data and should therefore be removed\n",
    "        output: bag of words\n",
    "        \"\"\"\n",
    "        doc = nlp(text)\n",
    "\n",
    "        bog = []\n",
    "\n",
    "        for token in doc:\n",
    "\n",
    "            # filter unwanted tokens\n",
    "            if token.is_stop:\n",
    "               continue\n",
    "\n",
    "            if token.is_punct:\n",
    "                continue\n",
    "            \n",
    "            if token.like_url:\n",
    "                continue\n",
    "            \n",
    "            if token.like_email:\n",
    "                continue\n",
    "            if token.is_space:\n",
    "                continue\n",
    "            \n",
    "            if token.lemma_ in ['\\n', ' ']:\n",
    "                continue\n",
    "            \n",
    "            if str(token) in search_words:\n",
    "                continue\n",
    "\n",
    "            # demojize\n",
    "            if token._.is_emoji:\n",
    "                token.lemma_ = f\"[{token._.emoji_desc.replace(' ', '_')}]\" \n",
    "                                       \n",
    "            bog.append(token.lemma_)                         \n",
    "        \n",
    "        return bog\n",
    "\n",
    "    def demojize(self, text: str, method: str = \"none\") -> str:\n",
    "        \"\"\" \n",
    "        text: Input string like a sentence\n",
    "        method:\n",
    "\n",
    "        - remove: remove emoji (happens automatically if no other method is given)\n",
    "        - none: do not remove emojis\n",
    "        - simple: replace emoji with [emoji] token\n",
    "        - text: replace emoji with text describin it\n",
    "        - token: like text but put [brackets] around the emoji token\n",
    "\n",
    "        output: string with emojis replaced by text\n",
    "        \"\"\"\n",
    "        \n",
    "        for token in nlp(text):\n",
    "                 \n",
    "            if token._.is_emoji:\n",
    "\n",
    "                if method==\"none\":\n",
    "                    continue\n",
    "\n",
    "                if method==\"simple\":\n",
    "                    text = text.replace(str(token), f\" [emoji] \") \n",
    "                    continue\n",
    "\n",
    "                if method==\"simple_text\":\n",
    "                    text = text.replace(str(token), f\" emoji \") \n",
    "                    continue\n",
    "\n",
    "                if method==\"text\":\n",
    "                    text = text.replace(str(token), f\" {token._.emoji_desc.replace(' ', '_')} \") \n",
    "                    continue\n",
    "\n",
    "                if method==\"token\":\n",
    "                    text = text.replace(str(token), f\" [{token._.emoji_desc.replace(' ', '_')}] \") \n",
    "                    continue\n",
    "\n",
    "                if method!=\"remove\":\n",
    "                    raise ValueError('You did not select a correct demojization method!')\n",
    "\n",
    "            # remove non-ascii\n",
    "            if token.is_ascii:\n",
    "                continue\n",
    "\n",
    "            # make an exeption for italian characters (e.g. Ã¨)\n",
    "            if token.is_alpha:\n",
    "                continue\n",
    "\n",
    "            text = text.replace(str(token), \"\") \n",
    "    \n",
    "        text_single_spaces = \" \".join(text.split())\n",
    "        \n",
    "        return text_single_spaces\n",
    "\n",
    "    def create_comment_list(self, dic: dict, cleaning=True, search_words: list = []) -> list:\n",
    "        \"\"\" \n",
    "        Creates a flat list of cleaned comments from a dictionary.\n",
    "\n",
    "        dic: dictionnary of comments\n",
    "        search_words: search words to pass to self.clean_text\n",
    "        \"\"\"\n",
    "\n",
    "        com_list = []\n",
    "\n",
    "        for key, video in dic.items():\n",
    "            for com in video[\"raw_comments\"]:\n",
    "                if cleaning:\n",
    "                    com_list.append(\" \".join(self.clean_text(com[\"text\"])))\n",
    "                else:\n",
    "                    com_list.append(com[\"text\"])\n",
    "\n",
    "        return com_list\n",
    "            \n",
    "    def create_edge_dictionary(self, comments: list) -> dict:\n",
    "        \"\"\" \n",
    "        Creates edges between two words that appear in the same comment. A weight is assigned according to the number of occurences of an edge in the dataset.\n",
    "\n",
    "        comments: list of comments where each comment is a list of words\n",
    "        output: dicitonary with edges as keys and weight as values\n",
    "        \"\"\"\n",
    "        \n",
    "        weights = {}\n",
    "\n",
    "        for com in comments:\n",
    "            for edge in combinations(com.split(), 2):\n",
    "\n",
    "                # if not self loop\n",
    "                if edge[0] != edge[1]:\n",
    "                    \n",
    "                    if edge not in weights.keys():\n",
    "                        weights[edge] = 1\n",
    "                    else:\n",
    "                        weights[edge] += 1\n",
    "                    \n",
    "        return weights\n",
    "\n",
    "    def edges_to_dataframe(self, edges: dict) -> pd.DataFrame:\n",
    "        \"\"\" \n",
    "        Creates data frame of edges from an edge dictionary (self.create_edge_dictionary). This can be saved to csv to use in gephi. In python better use the dicitionary as it is much faster.\n",
    "\n",
    "        edges: dictionary of edge\n",
    "        output: pd.DataFrame of edges\n",
    "        \"\"\"\n",
    "        df = pd.DataFrame({\"source\": [], \"target\": [], \"weigth\": []})\n",
    "\n",
    "        for edge, weight in edges.items():\n",
    "            row = pd.DataFrame({\"source\": [edge[0]], \"target\": [edge[1]], \"weigth\": [weight]})\n",
    "            df = pd.concat([df, row], ignore_index=True)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def create_comment_table(self, dic: dict, method: str = \"none\") -> pd.DataFrame:\n",
    "        \"\"\" \n",
    "        dic: dictionnary of comments\n",
    "        demojize: string defining the demojization method\n",
    "        output: pandas dataframe with columns id, name and comments\n",
    "        \"\"\"\n",
    "\n",
    "        com_list = []\n",
    "        id_list = []\n",
    "        name_list = []\n",
    "\n",
    "        for key, video in dic.items():\n",
    "\n",
    "            name = df.loc[df.id==key].name\n",
    "\n",
    "            for com in video[\"raw_comments\"]:\n",
    "                \n",
    "                demojized_text = self.demojize(com[\"text\"], method=method) \n",
    "                \n",
    "                com_list.append(demojized_text)\n",
    "                id_list.append(key)\n",
    "                name_list.extend(name)\n",
    "\n",
    "        df_out = pd.DataFrame({\"id\": id_list, \"name\": name_list, \"comments\": com_list})\n",
    "        return df_out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_str = \"Questa, Ã¨  Ã  una bellissima prova ð»ðð¿ @GiorgiaMeloni\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create comment lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create instance of our preprocessing class\n",
    "prepro = preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [54], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# loop over videos in spreadsheet and clean and add comments from d\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m com_list_all \u001b[39m=\u001b[39m prepro\u001b[39m.\u001b[39;49mcreate_comment_list(dic)\n",
      "Cell \u001b[0;32mIn [33], line 63\u001b[0m, in \u001b[0;36mpreprocessing.create_comment_list\u001b[0;34m(self, dic, cleaning, search_words)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39mfor\u001b[39;00m com \u001b[39min\u001b[39;00m video[\u001b[39m\"\u001b[39m\u001b[39mraw_comments\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m     62\u001b[0m     \u001b[39mif\u001b[39;00m cleaning:\n\u001b[0;32m---> 63\u001b[0m         com_list\u001b[39m.\u001b[39mappend(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclean_text(com[\u001b[39m\"\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m\"\u001b[39;49m])))\n\u001b[1;32m     64\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m         com_list\u001b[39m.\u001b[39mappend(com[\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "Cell \u001b[0;32mIn [33], line 14\u001b[0m, in \u001b[0;36mpreprocessing.clean_text\u001b[0;34m(self, text, search_words)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclean_text\u001b[39m(\u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m, search_words: \u001b[39mlist\u001b[39m \u001b[39m=\u001b[39m []) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mlist\u001b[39m:\n\u001b[1;32m      7\u001b[0m     \u001b[39m\"\"\" \u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39m    Cleans a string removing punctuation, emoji, stopwords, lemmatizazion, ...\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39m    output: bag of words\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     doc \u001b[39m=\u001b[39m nlp(text)\n\u001b[1;32m     16\u001b[0m     bog \u001b[39m=\u001b[39m []\n\u001b[1;32m     18\u001b[0m     \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m doc:\n\u001b[1;32m     19\u001b[0m \n\u001b[1;32m     20\u001b[0m         \u001b[39m# filter unwanted tokens\u001b[39;00m\n",
      "File \u001b[0;32m~/ds/3rd-semester/NetSci/netsci-venv/lib/python3.8/site-packages/spacy/language.py:1026\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     error_handler \u001b[39m=\u001b[39m proc\u001b[39m.\u001b[39mget_error_handler()\n\u001b[1;32m   1025\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1026\u001b[0m     doc \u001b[39m=\u001b[39m proc(doc, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcomponent_cfg\u001b[39m.\u001b[39;49mget(name, {}))  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m   1027\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1028\u001b[0m     \u001b[39m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[1;32m   1029\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE109\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/ds/3rd-semester/NetSci/netsci-venv/lib/python3.8/site-packages/spacy/pipeline/trainable_pipe.pyx:52\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/ds/3rd-semester/NetSci/netsci-venv/lib/python3.8/site-packages/spacy/pipeline/tok2vec.py:125\u001b[0m, in \u001b[0;36mTok2Vec.predict\u001b[0;34m(self, docs)\u001b[0m\n\u001b[1;32m    123\u001b[0m     width \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mget_dim(\u001b[39m\"\u001b[39m\u001b[39mnO\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    124\u001b[0m     \u001b[39mreturn\u001b[39;00m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39malloc((\u001b[39m0\u001b[39m, width)) \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m docs]\n\u001b[0;32m--> 125\u001b[0m tokvecs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpredict(docs)\n\u001b[1;32m    126\u001b[0m batch_id \u001b[39m=\u001b[39m Tok2VecListener\u001b[39m.\u001b[39mget_batch_id(docs)\n\u001b[1;32m    127\u001b[0m \u001b[39mfor\u001b[39;00m listener \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlisteners:\n",
      "File \u001b[0;32m~/ds/3rd-semester/NetSci/netsci-venv/lib/python3.8/site-packages/thinc/model.py:315\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X: InT) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m OutT:\n\u001b[1;32m    312\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[39m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/ds/3rd-semester/NetSci/netsci-venv/lib/python3.8/site-packages/thinc/layers/chain.py:55\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m---> 55\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39;49mis_train)\n\u001b[1;32m     56\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     57\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "File \u001b[0;32m~/ds/3rd-semester/NetSci/netsci-venv/lib/python3.8/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[0;32m~/ds/3rd-semester/NetSci/netsci-venv/lib/python3.8/site-packages/thinc/layers/chain.py:55\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m---> 55\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39;49mis_train)\n\u001b[1;32m     56\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     57\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "File \u001b[0;32m~/ds/3rd-semester/NetSci/netsci-venv/lib/python3.8/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[0;32m~/ds/3rd-semester/NetSci/netsci-venv/lib/python3.8/site-packages/thinc/layers/with_array.py:32\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, Xseq, is_train)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m     29\u001b[0m     model: Model[SeqT, SeqT], Xseq: SeqT, is_train: \u001b[39mbool\u001b[39m\n\u001b[1;32m     30\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[SeqT, Callable]:\n\u001b[1;32m     31\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(Xseq, Ragged):\n\u001b[0;32m---> 32\u001b[0m         \u001b[39mreturn\u001b[39;00m cast(Tuple[SeqT, Callable], _ragged_forward(model, Xseq, is_train))\n\u001b[1;32m     33\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(Xseq, Padded):\n\u001b[1;32m     34\u001b[0m         \u001b[39mreturn\u001b[39;00m cast(Tuple[SeqT, Callable], _padded_forward(model, Xseq, is_train))\n",
      "File \u001b[0;32m~/ds/3rd-semester/NetSci/netsci-venv/lib/python3.8/site-packages/thinc/layers/with_array.py:87\u001b[0m, in \u001b[0;36m_ragged_forward\u001b[0;34m(model, Xr, is_train)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_ragged_forward\u001b[39m(\n\u001b[1;32m     84\u001b[0m     model: Model[SeqT, SeqT], Xr: Ragged, is_train: \u001b[39mbool\u001b[39m\n\u001b[1;32m     85\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Ragged, Callable]:\n\u001b[1;32m     86\u001b[0m     layer: Model[ArrayXd, ArrayXd] \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mlayers[\u001b[39m0\u001b[39m]\n\u001b[0;32m---> 87\u001b[0m     Y, get_dX \u001b[39m=\u001b[39m layer(Xr\u001b[39m.\u001b[39;49mdataXd, is_train)\n\u001b[1;32m     89\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mbackprop\u001b[39m(dYr: Ragged) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Ragged:\n\u001b[1;32m     90\u001b[0m         \u001b[39mreturn\u001b[39;00m Ragged(get_dX(dYr\u001b[39m.\u001b[39mdataXd), dYr\u001b[39m.\u001b[39mlengths)\n",
      "File \u001b[0;32m~/ds/3rd-semester/NetSci/netsci-venv/lib/python3.8/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[0;32m~/ds/3rd-semester/NetSci/netsci-venv/lib/python3.8/site-packages/thinc/layers/chain.py:55\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m---> 55\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39;49mis_train)\n\u001b[1;32m     56\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     57\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "File \u001b[0;32m~/ds/3rd-semester/NetSci/netsci-venv/lib/python3.8/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[0;32m~/ds/3rd-semester/NetSci/netsci-venv/lib/python3.8/site-packages/thinc/layers/chain.py:55\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m---> 55\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39;49mis_train)\n\u001b[1;32m     56\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     57\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "File \u001b[0;32m~/ds/3rd-semester/NetSci/netsci-venv/lib/python3.8/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[0;32m~/ds/3rd-semester/NetSci/netsci-venv/lib/python3.8/site-packages/thinc/layers/maxout.py:49\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     47\u001b[0m W \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mget_param(\u001b[39m\"\u001b[39m\u001b[39mW\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m W \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39mreshape2f(W, nO \u001b[39m*\u001b[39m nP, nI)\n\u001b[0;32m---> 49\u001b[0m Y \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mgemm(X, W, trans2\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     50\u001b[0m Y \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39mreshape1f(b, nO \u001b[39m*\u001b[39m nP)\n\u001b[1;32m     51\u001b[0m Z \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39mreshape3f(Y, Y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], nO, nP)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# loop over videos in spreadsheet and clean and add comments from d\n",
    "\n",
    "com_list_all = prepro.create_comment_list(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a subdictionary via some fiter criteria from the spreadsheet (e.g. name)\n",
    "subdic_meloni = {k: dic[k] for k in df.loc[df[\"name\"]==\"Meloni\", \"id\"]}\n",
    "com_list_meloni = prepro.create_comment_list(subdic_melonib)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create & save edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create weighted edges. Two words are connected if they are in the same comment\n",
    "edges_all = prepro.create_edge_dictionary(com_list_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe of edges to be saved to csv for gephi\n",
    "edges_all_df = prepro.edges_to_dataframe(edges_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save edges to csv\n",
    "edges_all_df.to_csv(_data_path + \"edges_coms_all.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create comment table for LIWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_table_with_tokens = prepro.create_comment_table(dic, method=\"token\")\n",
    "comment_table_with_text = prepro.create_comment_table(dic, method=\"text\")\n",
    "comment_table_with_emoji_token = prepro.create_comment_table(dic, method=\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_table_with_emoji_text = prepro.create_comment_table(dic, method=\"simple_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7144986246215929094</td>\n",
       "      <td>Meloni</td>\n",
       "      <td>grazie adesso ho capito voto Conte</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7144986246215929094</td>\n",
       "      <td>Meloni</td>\n",
       "      <td>conteeeeeeee [smiling_face_with_3_hearts] [smi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7144986246215929094</td>\n",
       "      <td>Meloni</td>\n",
       "      <td>lei parla di lavoro mi dica oggi nella situazi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7144986246215929094</td>\n",
       "      <td>Meloni</td>\n",
       "      <td>a sentire tutti qui doveva passare Conte [smir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7144986246215929094</td>\n",
       "      <td>Meloni</td>\n",
       "      <td>bravissima [beaming_face_with_smiling_eyes]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id    name  \\\n",
       "0  7144986246215929094  Meloni   \n",
       "1  7144986246215929094  Meloni   \n",
       "2  7144986246215929094  Meloni   \n",
       "3  7144986246215929094  Meloni   \n",
       "4  7144986246215929094  Meloni   \n",
       "\n",
       "                                            comments  \n",
       "0                 grazie adesso ho capito voto Conte  \n",
       "1  conteeeeeeee [smiling_face_with_3_hearts] [smi...  \n",
       "2  lei parla di lavoro mi dica oggi nella situazi...  \n",
       "3  a sentire tutti qui doveva passare Conte [smir...  \n",
       "4        bravissima [beaming_face_with_smiling_eyes]  "
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'conteeeeeeeeð¥°ð¥°ð¥°ð¥°ð¥°ð¥° Giorgia e come la mettiamo quelli delle case popolari ð¡ð¡ð¡ð¡'"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic[\"7144986246215929094\"][\"raw_comments\"][1][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conteeeeeeee smiling_face_with_3_hearts smiling_face_with_3_hearts smiling_face_with_3_hearts smiling_face_with_3_hearts smiling_face_with_3_hearts smiling_face_with_3_hearts Giorgia e come la mettiamo quelli delle case popolari pouting_face pouting_face pouting_face pouting_face\n",
      "conteeeeeeee [smiling_face_with_3_hearts] [smiling_face_with_3_hearts] [smiling_face_with_3_hearts] [smiling_face_with_3_hearts] [smiling_face_with_3_hearts] [smiling_face_with_3_hearts] Giorgia e come la mettiamo quelli delle case popolari [pouting_face] [pouting_face] [pouting_face] [pouting_face]\n",
      "conteeeeeeee [emoji] [emoji] [emoji] [emoji] [emoji] [emoji] Giorgia e come la mettiamo quelli delle case popolari [emoji] [emoji] [emoji] [emoji]\n"
     ]
    }
   ],
   "source": [
    "print(comment_table_with_text.loc[comment_table_with_text.id==\"7144986246215929094\"].comments[1])\n",
    "print(comment_table_with_tokens.loc[comment_table_with_tokens.id==\"7144986246215929094\"].comments[1])\n",
    "print(comment_table_with_emoji_token.loc[comment_table_with_emoji_token.id==\"7144986246215929094\"].comments[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conteeeeeeee emoji emoji emoji emoji emoji emoji Giorgia e come la mettiamo quelli delle case popolari emoji emoji emoji emoji\n"
     ]
    }
   ],
   "source": [
    "print(comment_table_with_emoji_text.loc[comment_table_with_emoji_text.id==\"7144986246215929094\"].comments[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_table_with_text.to_excel(_data_path + \"comments_with_emoji_text.xlsx\", index=False)\n",
    "comment_table_with_tokens.to_excel(_data_path + \"comments_with_emoji_tokens.xlsx\", index=False)\n",
    "comment_table_with_emoji_token.to_excel(_data_path + \"comments_with_simple_emoji_token.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_table_with_emoji_text.to_excel(_data_path + \"comments_with_simple_emoji_text.xlsx\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\"> <i><b>Fin</b></i> </p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "netsci",
   "language": "python",
   "name": "netsci-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
